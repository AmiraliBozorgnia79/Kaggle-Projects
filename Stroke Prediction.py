# -*- coding: utf-8 -*-
"""Assignment 4-Kaggle

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-V9CdrXNuo-7rmKXZkiOWQhRC8tqF-CC
"""

# ==============================================================================
# ECE 657: Assignment 4 - Stroke Prediction
# STUDENT STARTER CODE
# ==============================================================================

# --- Import Essential Libraries ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --- Preprocessing & Pipeline ---
from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# --- Machine Learning Models ---
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb

# --- Set up global parameters ---
# Set a random seed for reproducibility
RANDOM_STATE = 42
# Define the cross-validation strategy
# Using StratifiedKFold is good practice for imbalanced datasets
CV_SPLITS = 5
cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)

# --- Set Plot Style ---
sns.set(style="whitegrid", palette="pastel", font_scale=1.1)
plt.rcParams['figure.figsize'] = [10, 6]

print("✅ Libraries imported and initial setup complete.")
print(f"Random State: {RANDOM_STATE}")
print(f"CV Folds: {CV_SPLITS}")

# ==============================================================================
# Part 0: Load Data
# ==============================================================================


try:
    train_df = pd.read_csv("train.csv")
    test_df = pd.read_csv("test.csv")
    sample_submission_df = pd.read_csv("sample_submission.csv")
    print("\nData loaded successfully!")
    print(f"Training data shape: {train_df.shape}")
    print(f"Test data shape: {test_df.shape}")
except FileNotFoundError as e:
    print(f"\n❌ ERROR: {e}. Please ensure all CSV files are in the correct directory.")
    # Create empty dataframes to avoid further errors in the notebook
    train_df = pd.DataFrame()
    test_df = pd.DataFrame()

#Summary of Statistics
print(train_df.describe())
print(train_df.info())
print(train_df.isnull().sum())
"from the results, the bmi has 141 missing values"

#Analyze the distribution of the stroke target variable
print("target variable", train_df['stroke'].value_counts())
#Visualization
sns.countplot(x='stroke', data=train_df)
plt.title('Distribution of Stroke')
plt.show()

#data distribution in key features
plt.figure(figsize=(8, 5))
sns.histplot(train_df, x='age',hue='stroke',kde=True, bins=30)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 5))
sns.histplot(train_df,x='avg_glucose_level',hue='stroke', kde=True, bins=30)
plt.title('Average Glucose Level Distribution')
plt.xlabel('Avg Glucose Level')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 5))
sns.histplot(train_df,x='bmi',hue='stroke',kde=True, bins=30)
plt.title('BMI Distribution')
plt.xlabel('BMI')
plt.ylabel('Frequency')
plt.show()


sns.kdeplot(data=train_df, x='avg_glucose_level', hue='stroke', fill=True, common_norm=False)
plt.title("Glucose Distribution by Stroke")
plt.xlabel("Average Glucose Level")
plt.ylabel("Density")
plt.show()

"""Baseline Modeling (Decision Tree):
Attempt to train a DecisionTreeClassifier after only imputing
BMI to observe the expected error. Baseline Evaluation: Correctly preprocess the data and evaluate the Decision Tree using 5-fold cross- validation
"""

#Step 1: Defining the input and target variable for baseline model (minimal preprocessing)
X_baseline = train_df.drop(['id', 'stroke'], axis=1)
y_baseline = train_df['stroke']

#Step 2: Impute missing values in BMI column using median strategy
imputer = SimpleImputer(strategy='median')
X_baseline['bmi'] = imputer.fit_transform(X_baseline[['bmi']])

#Step 3: Apply one-hot encoding to categorical variables using pandas
X_baseline_encoded = pd.get_dummies(X_baseline, drop_first=True)

#Step 4: Initialize and define baseline Decision Tree Classifier
baseline_model = DecisionTreeClassifier(random_state=42)

#Step 5: Evaluate baseline model using 5-fold cross-validation with AUC as metric
baseline_auc_scores = cross_val_score(baseline_model, X_baseline_encoded, y_baseline,
                                      cv=5, scoring='roc_auc')

#Step 6: Display AUC scores and mean performance
print("Baseline AUC Scores (BMI imputed, one-hot encoded):", baseline_auc_scores)
print("Baseline Mean AUC Score:", baseline_auc_scores.mean())
print("Baseline Mean AUC Score:", baseline_auc_scores.std())

"""Creating a pipeline to handle categorial feature and scaling numerical features as well as processing missing values"""

#Feauture lists classified to numerical and categorial
numerical_cols = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']
categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']


#Pipeline development
"numerical pipeline"
numerical_transformer = Pipeline([('imputer',SimpleImputer(strategy='median')),('scalar',StandardScaler())])

"categorial pipeline"
categorial_transformer=Pipeline([('encoder',OneHotEncoder(handle_unknown='ignore'))])

"Using column Transformer to combine categorial and numerical"
Preprocessor=ColumnTransformer([('num',numerical_transformer,numerical_cols),('cat',categorial_transformer,categorical_cols)])

#Defining the input and target variable
X_train=train_df.drop(['id','stroke'],axis=1)
y_train=train_df['stroke']

#Applying pipeline to the data
X_train_processed=Preprocessor.fit_transform(X_train)

#Converting the prcessed data to data frame
features_name=Preprocessor.get_feature_names_out()
X_train_processed=pd.DataFrame(X_train_processed,columns=features_name)

"""# Building Logistic regression and KNN with cross validation by using Stratified and class weight"""

#===========================================================
# Modules and CV/Scoring Definitions
#===========================================================
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as imbpipeline
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import ADASYN


# Define scoring metrics dictionary
scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, zero_division=0),
    'recall': make_scorer(recall_score, zero_division=0),
    'f1': make_scorer(f1_score, zero_division=0),
    'roc_auc': 'roc_auc'
}

# Define Stratified K-Fold (preserve class imbalance)
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

#===========================================================
# Logistic Regression (WITHOUT class_weight)
#===========================================================

Logistic_regression_unbalanced_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('Logistic', LogisticRegression())  # No class_weight
])

Logistic_unbalanced_score = cross_validate(Logistic_regression_unbalanced_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Logistic Regression WITHOUT class_weight")

for key in Logistic_unbalanced_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(Logistic_unbalanced_score[key])
        std = np.std(Logistic_unbalanced_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Logistic Regression with SMOTE
#===========================================================
Logistic_smote_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('Logistic', LogisticRegression())  # Optional: add class_weight='balanced' here too
])

Logistic_smote_score = cross_validate(Logistic_smote_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Logistic Regression WITH SMOTE")

for key in Logistic_smote_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(Logistic_smote_score[key])
        std = np.std(Logistic_smote_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Logistic Regression (WITH class_weight='balanced')
#===========================================================

Logistic_regression_balanced_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('Logistic', LogisticRegression(class_weight='balanced'))
])

Logistic_balanced_score = cross_validate(Logistic_regression_balanced_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Logistic Regression WITH class_weight='balanced'")

for key in Logistic_balanced_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(Logistic_balanced_score[key])
        std = np.std(Logistic_balanced_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Logistic Regression (WITH SMOTE + TomekLinks)
#===========================================================

Logistic_smote_tomek_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('resample', SMOTETomek(random_state=42)),
    ('Logistic', LogisticRegression())
])

Logistic_smote_tomek_score = cross_validate(Logistic_smote_tomek_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Logistic Regression WITH SMOTE + TomekLinks")

for key in Logistic_smote_tomek_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(Logistic_smote_tomek_score[key])
        std = np.std(Logistic_smote_tomek_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Logistic Regression (WITH ADASYN)
#===========================================================

Logistic_adasyn_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('resample', ADASYN(random_state=42)),
    ('Logistic', LogisticRegression())
])

Logistic_adasyn_score = cross_validate(Logistic_adasyn_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Logistic Regression WITH ADASYN")

for key in Logistic_adasyn_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(Logistic_adasyn_score[key])
        std = np.std(Logistic_adasyn_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# K-Nearest Neighbors (KNN) with SMOTE
#===========================================================
KNN_smote_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('KNN', KNeighborsClassifier())
])

KNN_smote_score = cross_validate(KNN_smote_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("K-Nearest Neighbors (KNN) WITH SMOTE")

for key in KNN_smote_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(KNN_smote_score[key])
        std = np.std(KNN_smote_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")



#===========================================================
# K-Nearest Neighbors (no class_weight support)
#===========================================================

KNN_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('KNN', KNeighborsClassifier())
])

KNN_score = cross_validate(KNN_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("K-Nearest Neighbors (KNN)")

for key in KNN_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(KNN_score[key])
        std = np.std(KNN_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")



#===========================================================
# K-Nearest Neighbors (KNN) WITH SMOTE + TomekLinks
#===========================================================

KNN_smote_tomek_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('resample', SMOTETomek(random_state=42)),
    ('KNN', KNeighborsClassifier())
])

KNN_smote_tomek_score = cross_validate(KNN_smote_tomek_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("K-Nearest Neighbors (KNN) WITH SMOTE + TomekLinks")

for key in KNN_smote_tomek_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(KNN_smote_tomek_score[key])
        std = np.std(KNN_smote_tomek_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")



#===========================================================
# K-Nearest Neighbors (KNN) WITH ADASYN
#===========================================================

KNN_adasyn_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('resample', ADASYN(random_state=42)),
    ('KNN', KNeighborsClassifier())
])

KNN_adasyn_score = cross_validate(KNN_adasyn_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("K-Nearest Neighbors (KNN) WITH ADASYN")

for key in KNN_adasyn_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(KNN_adasyn_score[key])
        std = np.std(KNN_adasyn_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

"""# Devloping advance models XGboost"""

#===========================================================
# XGBoost Classifier (Baseline - No Sampling)
#===========================================================
from xgboost import XGBClassifier

XGB_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('XGB', XGBClassifier(eval_metric='logloss', random_state=42))  # Removed use_label_encoder
])

XGB_score = cross_validate(XGB_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("XGBoost (Baseline - No Sampling)")

for key in XGB_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(XGB_score[key])
        std = np.std(XGB_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")


#===========================================================
# XGBoost with SMOTE
#===========================================================
XGB_smote_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('XGB', XGBClassifier(eval_metric='logloss', random_state=42))
])

XGB_smote_score = cross_validate(XGB_smote_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("XGBoost WITH SMOTE")

for key in XGB_smote_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(XGB_smote_score[key])
        std = np.std(XGB_smote_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# XGBoost with ADASYN
#===========================================================

XGB_adasyn_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('adasyn', ADASYN(random_state=42)),
    ('XGB', XGBClassifier(eval_metric='logloss', random_state=42))
])

XGB_adasyn_score = cross_validate(XGB_adasyn_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("XGBoost WITH ADASYN")

for key in XGB_adasyn_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(XGB_adasyn_score[key])
        std = np.std(XGB_adasyn_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")


#===========================================================
# XGBoost with SMOTE + TomekLinks
#===========================================================

XGB_smote_tomek_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote_tomek', SMOTETomek(random_state=42)),
    ('XGB', XGBClassifier(eval_metric='logloss', random_state=42))
])

XGB_smote_tomek_score = cross_validate(XGB_smote_tomek_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("XGBoost WITH SMOTE + TomekLinks")

for key in XGB_smote_tomek_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(XGB_smote_tomek_score[key])
        std = np.std(XGB_smote_tomek_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# XGBoost Manually tuned
#===========================================================
# Calculate scale_pos_weight = (num_negatives / num_positives)
neg, pos = np.bincount(y_train)
scale_pos_weight = neg / pos

# Define tuned XGBoost model
xgb_best_model = XGBClassifier(
    n_estimators=1200,
    learning_rate=0.0075,
    max_depth=5,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=2,
    reg_alpha=1,
    reg_lambda=5,
    scale_pos_weight=scale_pos_weight,
    eval_metric='logloss',
    random_state=42
)

# Create pipeline
XGB_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('xgb', xgb_best_model)
])

# Evaluate with Stratified CV
XGB_best_score = cross_validate(XGB_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

# Print results
print('#==============================================')
print("XGBoost WITH scale_pos_weight and Tuning")

for key in XGB_best_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(XGB_best_score[key])
        std = np.std(XGB_best_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

"""# Developing Random Forest Algorithms"""

from sklearn.ensemble import RandomForestClassifier
#===========================================================
# Random Forest Classifier (Baseline - No Sampling)
#===========================================================
RF_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('RF', RandomForestClassifier(random_state=42))
])

RF_score = cross_validate(RF_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Random Forest (Baseline - No Sampling)")

for key in RF_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(RF_score[key])
        std = np.std(RF_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Random Forest with SMOTE
#===========================================================
RF_smote_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('RF', RandomForestClassifier(random_state=42))
])

RF_smote_score = cross_validate(RF_smote_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Random Forest WITH SMOTE")

for key in RF_smote_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(RF_smote_score[key])
        std = np.std(RF_smote_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Random Forest with ADASYN
#===========================================================
RF_adasyn_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('adasyn', ADASYN(random_state=42)),
    ('RF', RandomForestClassifier(random_state=42))
])

RF_adasyn_score = cross_validate(RF_adasyn_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Random Forest WITH ADASYN")

for key in RF_adasyn_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(RF_adasyn_score[key])
        std = np.std(RF_adasyn_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Random Forest with SMOTE + TomekLinks
#===========================================================
RF_smote_tomek_pipeline = imbpipeline([
    ('preprocessor', Preprocessor),
    ('smote_tomek', SMOTETomek(random_state=42)),
    ('RF', RandomForestClassifier(random_state=42))
])

RF_smote_tomek_score = cross_validate(RF_smote_tomek_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Random Forest WITH SMOTE + TomekLinks")

for key in RF_smote_tomek_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(RF_smote_tomek_score[key])
        std = np.std(RF_smote_tomek_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

#===========================================================
# Random Forest with class_weight='balanced'
#===========================================================
RF_balanced_pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('RF', RandomForestClassifier(class_weight='balanced', random_state=42))
])

RF_balanced_score = cross_validate(RF_balanced_pipeline, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Random Forest WITH class_weight='balanced'")

for key in RF_balanced_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(RF_balanced_score[key])
        std = np.std(RF_balanced_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

"""# XGBoost and Logistic Regression (LR) — using a soft voting ensemble"""

from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier

#======================
# Further Tuned XGBoost Model
#======================
xgb_boosted_model = XGBClassifier(
    n_estimators=1500,
    learning_rate=0.005,
    max_depth=6,
    subsample=0.65,
    colsample_bytree=0.65,          # Same here
    gamma=4,
    reg_alpha=2,                    # L1 regularization
    reg_lambda=6,                   # L2 regularization
    min_child_weight=5,             # More conservative splits
    scale_pos_weight=scale_pos_weight,  # To address class imbalance
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42
)

#======================
# Pipelines for Voting
#======================
xgb_clf = Pipeline([
    ('preprocessor', Preprocessor),
    ('xgb', xgb_boosted_model)
])

lr_clf = Pipeline([
    ('preprocessor', Preprocessor),
    ('lr', LogisticRegression(
        class_weight='balanced',
        solver='liblinear',
        random_state=42
    ))
])

#======================
# Soft Voting Ensemble
#======================
voting_clf = VotingClassifier(
    estimators=[
        ('xgb', xgb_clf),
        ('lr', lr_clf)
    ],
    voting='soft',
    n_jobs=-1
)

#======================
# Evaluate Ensemble
#======================
voting_score = cross_validate(voting_clf, X_train, y_train, cv=cv_strategy, scoring=scoring)

print('#==============================================')
print("Soft Voting Ensemble (Boosted XGBoost + Logistic Regression)")

for key in voting_score.keys():
    if key.startswith('test_'):
        metric = key.replace('test_', '')
        mean = np.mean(voting_score[key])
        std = np.std(voting_score[key])
        print(f"{metric.upper()} → Mean: {mean:.4f}, Std: {std:.4f}")

import joblib

# Fit model on entire training set if not already fitted
voting_clf.fit(X_train, y_train)

# Save model
joblib.dump(voting_clf, 'final_model.pkl')
print("✅ Model saved as 'final_model.pkl'")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

#===========================
# Fit Final Model on Full Training Set
#===========================
voting_clf.fit(X_train, y_train)

#===========================
# Evaluate on Training Set
#===========================
y_train_pred = voting_clf.predict(X_train)
y_train_prob = voting_clf.predict_proba(X_train)[:, 1]

# Metrics
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_recall = recall_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred)
train_roc_auc = roc_auc_score(y_train, y_train_prob)
train_cm = confusion_matrix(y_train, y_train_pred)

print("#==============================================")
print("✅ Final Model Evaluation on Training Set")
print(f"ACCURACY → {train_accuracy:.4f}")
print(f"PRECISION → {train_precision:.4f}")
print(f"RECALL → {train_recall:.4f}")
print(f"F1 SCORE → {train_f1:.4f}")
print(f"ROC_AUC → {train_roc_auc:.4f}")
print("CONFUSION MATRIX:")
print(train_cm)
print("#==============================================")

# ✅ Step 1: Load Test Data
#===========================================================
test_df = pd.read_csv("test.csv")  # Adjust path if needed
test_ids = test_df['id']

#===========================================================
# ✅ Step 2: Drop ID column before prediction
#===========================================================
X_test = test_df.drop(columns=['id'])  # Drop only 'id' if that's the only non-feature

#===========================================================
# ✅ Step 3: Fit Voting Classifier on Full Train Data
#===========================================================
voting_clf.fit(X_train, y_train)

#===========================================================
# ✅ Step 4: Predict Stroke Probabilities
#===========================================================
test_probs = voting_clf.predict_proba(X_test)[:, 1]  # P(stroke=1)

#===========================================================
# ✅ Step 5: Create Submission File
#===========================================================
submission_df = pd.DataFrame({
    'id': test_ids,
    'stroke': test_probs
})

submission_df.to_csv("submission4.csv", index=False)
print("✅ submission.csv created successfully!")

import joblib
import pandas as pd

# Load your saved model
final_model = joblib.load('final_model.pkl')

# Collect input from user
def get_user_input():
    print("Please enter the following information:")
    gender = input("Gender (Male/Female/Other): ")
    age = float(input("Age: "))
    hypertension = int(input("Hypertension (0 or 1): "))
    heart_disease = int(input("Heart Disease (0 or 1): "))
    ever_married = input("Ever Married (Yes/No): ")
    work_type = input("Work Type (Private/Self-employed/Govt_job/children/Never_worked): ")
    residence_type = input("Residence Type (Urban/Rural): ")
    avg_glucose_level = float(input("Average Glucose Level: "))
    bmi = float(input("BMI: "))
    smoking_status = input("Smoking Status (formerly smoked/never smoked/smokes/Unknown): ")

    # Create input sample
    sample = pd.DataFrame([{
        'gender': gender,
        'age': age,
        'hypertension': hypertension,
        'heart_disease': heart_disease,
        'ever_married': ever_married,
        'work_type': work_type,
        'Residence_type': residence_type,
        'avg_glucose_level': avg_glucose_level,
        'bmi': bmi,
        'smoking_status': smoking_status
    }])

    return sample

# Prediction function
def predict(sample):
    probability = final_model.predict_proba(sample)[0][1]
    print(f"\n🔎 Predicted Stroke Probability: {probability:.4f}")
    if probability > 0.5:
        print("⚠️ High stroke risk.")
    else:
        print("✅ Low stroke risk.")

# Main
if __name__ == "__main__":
    sample_input = get_user_input()
    predict(sample_input)